<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>FastCAD</title>
<!--     <link rel="shortcut icon" type="image/jpg" href="img/logo.ico" /> -->
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>
</head>

<body>
    <nav class="navbar is-light" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item" href="https://www.cam.ac.uk/">
                    <img src="img/Cam_bw.png" alt="University of Cambridge" style="height: 2.0rem;">
                </a>
                <a class="navbar-item" href="https://www.qualcomm.com/">
                    <img src="img/qualcomm_transparent.png" alt="Qualcomm" style="height: 2.0rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-end">
                    <a class="navbar-item navbar-right" href="https://eccv.ecva.net/">
                        <img src="img/eccv-2024.png" alt="ECCV 2024" style="height: 2.0rem;">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
<!--             <div class="columns">
                <div class="column align-center is-hidden-mobile">
                    <img src="img/irondepth/irondepth-pad.png" />
                </div>
                <div class="column align-center is-hidden-desktop is-hidden-tablet">
                    <img src="img/irondepth/irondepth.png" />
                </div>
            </div> -->
            <h1 class="title is-2 is-size-3-mobile is-spaced has-text-centered">
                FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos
            </h1>
            <p class="subtitle is-5 has-text-centered has-text-grey">
                ECCV 2024
            </p>
            <p class="subtitle is-6 has-text-centered authors mt-5" style="line-height: 1.5;">
                <span>
                    <a href="https://www.florianlanger.co.uk">Florian Langer</a><sup>1,2</sup>
                </span>
                <span>
                    <a href="https://jihongju.github.io/">Jihong Ju</a><sup>2</sup>
                </span>
                <span>
                    <a href="https://gdikov.me/">Georgi Dikov</a><sup>2</sup>
                </span>
                <span>
                    <a href="https://scholar.google.com/citations?user=IVFjU50AAAAJ&hl=en">Gerhard Reitmayr</a><sup>2</sup>
                </span>
                <span>
                    <a href="https://scholar.google.com/citations?user=989WL-wAAAAJ&hl=en">Mohsen Ghafoorian</a><sup>2</sup>
                </span>
            </p>
            <p class="subtitle is-6 has-text-centered institutions mt-5" style="line-height: 1.5;">
                <span style="margin-right: 20px;">University of Cambridge<sup>1</sup></span>
                <span>XR Labs, Qualcomm Technologies, Inc.<sup>2</sup></span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <!-- <a href="https://github.com/baegwangbin/IronDepth/raw/main/paper.pdf" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a> -->
            <a href="https://arxiv.org/abs/2403.15161" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <!-- <a href="https://github.com/florianlanger/sparc" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
            </a> -->
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <!-- Note about code availability and consultancy -->
<p class="has-text-centered has-text-grey">
    <em>Unfortunately, we are not able to release the code for FastCAD. However, the first author, Florian Langer, is happy to answer any questions via email at <a href="mailto:fml35@cam.ac.uk">fml35@cam.ac.uk</a> or by setting up a short call. Florian Langer is also offering consultancy services in computer vision, so if that is of interest, please feel free to reach out or visit his website: <a href="https://www.florianlanger.co.uk">florian.langer.co.uk</a>.</em>
</p>
            <figure class="image">
                <img src="img/intro_v10.png" style="margin-top: 30px; margin-bottom: 40px;"/>
            </figure>
            <h1 class="title is-4">
                TL;DR
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    <ul>
                        <li>We train our network FastCAD to <b>simultaneously retrieve and align CAD models</b> for all objects in a scene.</li>
                        <li>We achieve <b>high-quality shape retrievals</b> by learning an embedding space in a contrastive learning framework and distilling those embeddings into FastCAD.</li>
                        <li>FastCAD can predict a CAD model-based scene representation in just <b>50 ms</b> when using a <b>point cloud</b> as the input and <b>100 ms</b> when using a <b>video</b>. In doing so it is significnatly faster and more accurate than competing methods.</li>
                    </ul>
                </p>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop is-hidden-mobile">
            <h1 class="title is-4">
                Demo
            </h1>
            <iframe style="display: block; margin: auto;" width="768" height="432" src="https://www.youtube.com/embed/KIcFRBu0CiE?si=zjoZc2W-OiyUyRq7" frameborder="0" allowfullscreen></iframe>
        </div>
        <div class="container is-max-desktop is-hidden-desktop is-hidden-tablet">
            <h1 class="title is-4">
                Demo
            </h1>
            <iframe style="display: block; margin: auto;" width="300" height="200" src="https://www.youtube.com/embed/KIcFRBu0CiE?si=zjoZc2W-OiyUyRq7" frameborder="0" allowfullscreen></iframe>
        </div>
    </section>
    <!-- <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                Single image surface normal estimation and depth estimation are closely related problems as the former can be calculated from the latter. 
                However, the surface normals computed from the output of depth estimation methods are significantly less accurate than the surface normals directly estimated by networks. 
                To reduce such discrepancy, we introduce a novel framework that uses surface normal and its uncertainty to recurrently refine the predicted depth-map. 
                The depth of each pixel can be propagated to a query pixel, using the predicted surface normal as guidance. 
                We thus formulate depth refinement as a classification of choosing the neighboring pixel to propagate from. 
                Then, by propagating to sub-pixel points, we upsample the refined, low-resolution output. 
                The proposed method shows state-of-the-art performance on NYUv2 and iBims-1 - both in terms of depth and normal. 
                Our refinement module can also be attached to the existing depth estimation methods to improve their accuracy. 
                We also show that our framework, only trained for depth estimation, can also be used for depth completion. 
                The code is available at https://github.com/baegwangbin/IronDepth.
            </div>
        </div>
    </section> -->
    <!-- <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Motivation
            </h1>
             <figure class="image">
                <img src="img/irondepth/normal.png"/>
            </figure>
            <div class="content has-text-justified-desktop">
                <p>
                    Render-and-Compare allows for precise CAD model alignments. However, traditionally it is very slow and requires a good initialisation.
                    It is slow because
                    <ol>
                        <li>it requires to render full objects.</li>
                        <li>it then requires to process fully-rendered images.</li>
                        <li>a similarity function between the image and the render is maximised via gradient descent which requires a large number of iterations (100s to 1000).</li>
                    </ol>
                    It requires a good initialisation because
                    <ol start="4">
                        <li>maximising the similarity function may get stuck in local optima.</li>
                    </ol>
                    We address 1. and 2. by only sampling a set of sparse points and surface normals from the CAD model to be rendered and then processing only those sparse inputs as opposed to a full image.
                    We address 3. and 4. by using a network to directly predict pose updates rather than a similarity function which reduces the number of iterations needed to just 3 and simultaneously makes the network robust to object initialisations.
                </p>
                <p> In our <a href="https://github.com/baegwangbin/surface_normal_uncertainty">previous work</a>, we estimated the <b>aleatoric uncertainty in surface normal estimation</b>, and used it to improve the quality of prediction for small structures and object boundaries.
                    We believe that the estimated surface normal (and its uncertainty) can be useful in various computer vision tasks. 
                    In our <a href="https://arxiv.org/abs/2210.01044">recent work</a>, we showed that it can be used to <b>align CAD models</b> to the objects in the image.
                    In this work, we show that it can improve <b>monocular depth estimation</b>.
                </p>
            </div>
        </div>
    </section> -->
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Method
            </h1>
            <figure class="image">
                <img src="img/method_v21.png" style="margin-top: 10px; margin-bottom: 30px;"/>
            </figure>
            <div class="content has-text-justified-desktop">
                <h1 class="title is-6">
                    CAD Retrieval and Alignment
                </h1>
                <p></p>
                <p>
                    The input to FastCAD is a point cloud, which may be derived from an RGB-D scan or a noisy
scene reconstruction obtained by applying a neural reconstruction method to a video.
For each detected object we predict its class <b>p</b>, oriented bounding box parameters <b>b</b>,
front-facing side classification <b>f</b> and shape embedding vector <b>w</b>. The front-facing side prediction allows us to choose between the four
possible orientations when aligning the CAD model within the oriented bounding box.
                </p>
                <h1 class="title is-6">
                    Learned Embedding Space
                </h1>
                <p></p>
                <p>
                    We learn a shape embedding space using a contrastive learning setup with two new auxiliary tasks.
                    We use  cropped object point clouds from the RGB-D scan as the anchors and associate the point cloud from the annotated CAD model 
                    as the positive example and a point cloud from a randomly sampled CAD model of the same category as the negative example.
                    These three point clouds are passed through an encoder to produce embeddings <b>A</b> (anchor), <b>P</b> (positive) and <b>N</b> (negative) to which we apply a triplet loss.
                    Further, we introduce two auxiliary tasks, performing foreground-background segmentation and estimating the shape similarity between the positive and the negative CAD model, which improve the quality of the embeddings.
                </p>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Results
            </h1>
            <div class="content has-text-justified-desktop">
                <p></p>
                <p>We train and evaluate FastCAD on the Scan2CAD dataset which annotates ScanNet with CAD models. Visually inspecting our predictions we find that FastCAD is able to produce accurate CAD model retrievals and alignments from both point clouds and videos.</p>
            </div>
            <figure class="image">
                <img src="img/qualitative_visualisations_v4.png"/>
            </figure>
            <div class="content has-text-justified-desktop">
                <p></p>
                <p> Quantitatively we find that FastCAD  outperforms competing methods, particularly in the video setting, while being significantly faster.</p>
            </div>
            <figure class="image">
                <img src="img/quantitative_results.png"/>
            </figure>
            <!-- <figure class="image">
                <img src="img/irondepth/depth_refinement.png"/>
            </figure>
            <div class="content has-text-justified-desktop">
                <p></p>
                <p>IronDepth can also be used as a post-processing tool to improve the accuracy of the existing depth estimation methods.
                </p>
            </div>
            <figure class="image">
                <img src="img/irondepth/depth_completion.png"/>
            </figure>
            <div class="content has-text-justified-desktop">
                <p></p>
                <p> Since we refine the predicted depth-map by propagating information between pixels, we can seamlessly apply our method to a scenario where sparse depth measurements are available (i.e. depth completion setup). 
                    Given a sparse depth measurement, we can add anchor points by fixing the depth for the pixels with measurement. 
                    The information provided for the anchor points (i.e. the measured depth) can be propagated to neighboring pixels, making the overall prediction more accurate.
                </p>
            </div> -->
        </div>
    </section>
    <!-- <section class="section pt-0">
        <div class="container is-max-desktop is-hidden-mobile">
            <h1 class="title is-4">
                Video Presentation
            </h1>
            <iframe style="display: block; margin: auto;" width="768" height="432" src="https://www.youtube.com/embed/x71KCZUC9SQ" frameborder="0" allowfullscreen></iframe>
        </div>
        <div class="container is-max-desktop is-hidden-desktop is-hidden-tablet">
            <h1 class="title is-4">
                Video Presentation
            </h1>
            <iframe style="display: block; margin: auto;" width="300" height="200" src="https://www.youtube.com/embed/x71KCZUC9SQ" frameborder="0" allowfullscreen></iframe>
        </div>
    </section> -->
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
@inproceedings{FastCAD,
author = {Florian Langer and Jihong Ju and Georgi Dikov and Gerhard Reitmayr and Mohsen Ghafoorian},
title = {FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos},
booktitle = {Proc. European Conference on Computer Vision (ECCV)},
month = {October},
year = {2024},
address={Milan}
}
</pre>
        </div>
    </section>
    <!-- <footer class="footer">
        <div class="content has-text-centered">
            <p>
                <img src="img/logo/Cam_bw.png" class="mt-5" alt="University of Cambridge" style="height: 2rem;">
            </p>
        </div>
    </footer> -->
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>